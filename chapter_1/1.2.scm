1.2 Procedures and the Processes They Generate

## Overview
This chapter is mostly about how programs respond to input size and 
visualizing the process of a procedure.  It talks about the "shapes"
of processes generated by a procedure and how these processes consume
computational resources (eg. memory / cpu cycles).  

Much of the chapter is dedicated to teasing out the differences in a
procedure that solves a problem though recursion vs. iteration.  There's
a lot of discussion of what we now call Big "O" notation (although it's 
called "theta" notation in the book).

1.2.1 Linear Recursion & Iteration
A comparison of a solving a factorial via recursion and via iteration.
The recursive procedure calls itself until it reaches a base case (a
factorial of 1 is 1), where the recursive procedure is essentially a
for loop that calls another procedure the correct number of times.

While both require the same number of steps to solve for any given
factorial, the recursive example requires more memory to solve because
it must retain all of the previous steps to solve for the next step.  
Compare that with the iterative case, where the variables of each step
summarize the state.  So the interpreter only needs to keep track of those
state variables to solve the procedure.

Of note: a procedure written in linear recursion almost always has an 
iterative counterpart - just take the recursive step and turn it into
it's own procedure and then execute in a loop.  The program ends when
it satisfies the base case of the loop.

Exercise 1.9

Exercise 1.10

1.2.2 Tree Recursion
Example using the Fibonacci squence (where each number is the sum of the 
proeceeding two).

When implemented recursively, the process could be drawn as a tree.  It shows
how computationally expensive it is to calculate a sequence of Fibonancci numbers 
this way, as so much work is duplicated.  The number of steps required grows exponentially
with the size of the input.

Compare that with the method that uses linear iteration, which requires far fewer
steps.

There's a counting change example which is worth reviewing.

Exercise 1.11

Exercise 1.12

Exercise 1.13

1.2.3 Orders of Growth
As inputs to a procedure become larger, we can obtain a gross measure of the resources
required by the procedure.  (Often called Big "O" notation).

In order of efficiency:
O(1) = constant time, (eg a hash lookup)
O(log n) = logarithmic time, such as a binary search tree
O(n) = linear time, (like looking though an array)
O(n2) = quadratic time (like two nested loops - bad)




Exercise 1.14
- Worth working out?

Exercise 1.15





